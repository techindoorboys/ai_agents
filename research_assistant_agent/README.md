# ğŸ“š Research Assistant with Citations (Gemini + LangGraph)

A simple yet powerful **AI Research Assistant** that can:
- Answer research questions
- Analyze PDF research papers from a URL
- Generate **abstracts, summaries, key insights**
- Provide **citations / references**
- Orchestrate logic using **LangGraph**
- Use **Gemini LLM** end-to-end
- Run fully inside **GitHub Codespaces**

---

## âœ¨ Features

- ğŸ” Web search using DuckDuckGo (via `ddgs`)
- ğŸ“„ PDF analysis directly from URL
- ğŸ§  Gemini LLM (latest Google GenAI SDK)
- ğŸ•¸ LangGraph for agent orchestration
- âš¡ FastAPI backend
- ğŸ¨ Streamlit frontend
- ğŸ” Environment variable support with `dotenv`
- ğŸ“¦ Dependency management using `uv`

---

## ğŸ—ï¸ Architecture Overview
```yaml
Streamlit UI
|
v
FastAPI Backend
|
v
LangGraph Agent
|
â”œâ”€â”€ DuckDuckGo Search Tool (ddgs)
â”œâ”€â”€ PDF Reader Tool
â””â”€â”€ Gemini LLM
```

---

## ğŸ“‚ Project Structure
```yaml
research-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py # FastAPI app + LangGraph wiring
â”‚ â”œâ”€â”€ agent.py # Gemini LLM wrapper
â”‚ â”œâ”€â”€ tools.py # DuckDuckGo + PDF tools
â”‚ â”œâ”€â”€ prompts.py # Prompt templates
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ app.py # Streamlit UI
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ .env # Environment variables
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ§° Tech Stack

| Component | Tool |
|--------|------|
| Language | Python 3.11+ |
| LLM | Gemini (`google-genai`) |
| Agent Orchestration | LangGraph |
| Backend | FastAPI |
| Frontend | Streamlit |
| Search | DuckDuckGo (`ddgs`) |
| PDF Parsing | pypdf |
| Env Management | python-dotenv |
| Package Manager | uv |

---

## ğŸ”‘ Environment Setup

Create a `.env` file in the project root:

```bash
GOOGLE_API_KEY=your_gemini_api_key_here
You can get a Gemini API key from Google AI Studio.
```

ğŸ Python Environment (Using UV) and ğŸ“¦ Install Dependencies

```bash
uv venv
source .venv/bin/activate
uv pip install -r backend/requirements.txt

uv pip install -r frontend/requirements.txt
```

â–¶ï¸ Running the Application (GitHub Codespaces)
1ï¸âƒ£ Start the Backend (FastAPI)

```bash
cd backend
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

FastAPI will be available on port 8000

Codespaces automatically exposes the port

2ï¸âƒ£ Start the Frontend (Streamlit)
Open a new terminal:

```bash
cd frontend
streamlit run app.py --server.address 0.0.0.0 --server.port 8501
```

Streamlit will be available on port 8501

ğŸ§ª Example Inputs
ğŸ” Research Question

```pgsql
Impact of large language models on healthcare diagnostics
```

ğŸ“„ PDF URL
```arduino
https://arxiv.org/pdf/2303.08774.pdf
```
Check the â€œIs this a PDF URL?â€ box when using a PDF.


ğŸ§  How LangGraph Works in This Project
The agent follows a simple routed flow:

```java
START
  |
  v
Router Node
  |
  â”œâ”€â”€ Search Node (DuckDuckGo)
  â””â”€â”€ PDF Node (PDF Reader)
        |
        v
     LLM Node (Gemini)
        |
       END
```

Key Design Rules
All LangGraph nodes return state dictionaries

Routing logic is handled via add_conditional_edges

LLM is always called with validated content

âš ï¸ Common Issues & Fixes
âŒ Expected dict, got search
* Ensure routing functions return strings
* Ensure nodes return dictionaries only

âŒ .json() decode error in Streamlit
* Backend returned an error or HTML
* Check FastAPI logs for traceback

âŒ Empty LLM response
* Ensure search or PDF content is not empty
* Guard against empty search results

ğŸš€ Future Improvements
* Add citation numbering [1] [2] [3]
* Convert output to structured JSON
* Add RAG with embeddings
* Enable streaming responses
* Export results to PDF or Markdown
* Deploy to Cloud Run / Fly.io

ğŸ“œ License
* MIT License â€“ free to use, modify, and distribute.

ğŸ™Œ Acknowledgements
* LangGraph & LangChain
* Google Gemini
* DuckDuckGo
* Streamlit & FastAPI
